{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m###################\n",
      "# Loading Dataset #\n",
      "###################\u001b[0m\n",
      "\u001b[32m##################\n",
      "# Dataset Loaded #\n",
      "##################\u001b[0m\n",
      "\u001b[33m#####################################\n",
      "# Dataset preparation and splitting #\n",
      "#####################################\u001b[0m\n",
      "\u001b[32m####################\n",
      "# Dataset Splitted #\n",
      "####################\u001b[0m\n",
      "\u001b[32m#######################\n",
      "# Pipeline Configured #\n",
      "#######################\u001b[0m\n",
      "\u001b[33m######################\n",
      "# Training SVM Model #\n",
      "######################\u001b[0m\n",
      "\u001b[32m#####################\n",
      "# SVM Model Trained #\n",
      "#####################\u001b[0m\n",
      "\u001b[33m########################\n",
      "# Evaluating SVM Model #\n",
      "########################\u001b[0m\n",
      "\u001b[33m##################################\n",
      "# SVM Accuracy:0.994797368080323 #\n",
      "##################################\u001b[0m\n",
      "\u001b[32m#####\n",
      "# {'              precision    recall  f1-score   support\\n\\n         0.0       0.85      0.73      0.78       512\\n         1.0       0.93      0.77      0.84       540\\n         2.0       0.88      0.67      0.76       506\\n         3.0       1.00      1.00      1.00     83399\\n\\n    accuracy                           0.99     84957\\n   macro avg       0.91      0.79      0.85     84957\\nweighted avg       0.99      0.99      0.99     84957\\n'} #\n",
      "#####\u001b[0m\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.73      0.78       512\n",
      "         1.0       0.93      0.77      0.84       540\n",
      "         2.0       0.88      0.67      0.76       506\n",
      "         3.0       1.00      1.00      1.00     83399\n",
      "\n",
      "    accuracy                           0.99     84957\n",
      "   macro avg       0.91      0.79      0.85     84957\n",
      "weighted avg       0.99      0.99      0.99     84957\n",
      "\n",
      "\u001b[32m#######################\n",
      "# SVM Model Evaluated #\n",
      "#######################\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from colorama import Fore, Style\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import pickle\n",
    "\n",
    "def print_decorative_log(message, color=Fore.BLUE, style=Style.RESET_ALL):\n",
    "    line_length = len(message) + 4  # Length of the message plus padding on both sides\n",
    "    decorative_line = \"#\" * line_length\n",
    "    print(color + decorative_line)\n",
    "    print(f\"# {message} #\")\n",
    "    print(decorative_line + style)\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "print_decorative_log(\"Loading Dataset\", Fore.YELLOW)\n",
    "df = pd.read_csv('merged_dataset.csv')\n",
    "# Define the column names\n",
    "columns = ['mfcc_' + str(i) for i in range(1, 301)] + ['label']\n",
    "\n",
    "# Assign the column names to the DataFrame\n",
    "df.columns = columns\n",
    "\n",
    "print_decorative_log(\"Dataset Loaded\", Fore.GREEN)\n",
    "\n",
    "# Configure TensorFlow to use GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# # Dropping rows with label equal to 3\n",
    "# df = df[df['label'] != 3]\n",
    "\n",
    "print_decorative_log(\"Dataset preparation and splitting\", Fore.YELLOW)\n",
    "X = df.drop('label', axis=1).values.astype(np.float32)  # Features\n",
    "#y = df['label'].values  # Labels\n",
    "y = df['label'].values.astype(np.float32)  # Labels\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print_decorative_log(\"Dataset Splitted\", Fore.GREEN)\n",
    "# Define a standard scaler\n",
    "scaler = StandardScaler()\n",
    "# Normalize the feature values using StandardScaler\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# SVM Model\n",
    "svm_model = SVC()\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', svm_model)\n",
    "])\n",
    "\n",
    "print_decorative_log(\"Pipeline Configured\", Fore.GREEN)\n",
    "\n",
    "print_decorative_log(\"Training SVM Model\", Fore.YELLOW)\n",
    "# Fit the SVM model\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print_decorative_log(\"SVM Model Trained\", Fore.GREEN)\n",
    "\n",
    "print_decorative_log(\"Evaluating SVM Model\", Fore.YELLOW)\n",
    "# Make predictions\n",
    "y_pred = svm_pipeline.predict(X_test)\n",
    "accuracy = svm_pipeline.score(X_test, y_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print_decorative_log(\"SVM Accuracy:\" + str(accuracy), Fore.YELLOW)\n",
    "# Print the classification report\n",
    "print_decorative_log({report}, Fore.GREEN)\n",
    "print()\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "print_decorative_log(\"SVM Model Evaluated\", Fore.GREEN)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.994797368080323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.73      0.78       512\n",
      "         1.0       0.93      0.77      0.84       540\n",
      "         2.0       0.88      0.67      0.76       506\n",
      "         3.0       1.00      1.00      1.00     83399\n",
      "\n",
      "    accuracy                           0.99     84957\n",
      "   macro avg       0.91      0.79      0.85     84957\n",
      "weighted avg       0.99      0.99      0.99     84957\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"SVM Accuracy: {accuracy}\")\n",
    "print(report)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m######################\n",
      "# SVM Model Exported #\n",
      "######################\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export the trained SVM model to a pickle file\n",
    "with open('epilepsy_prediction_model.pkl', 'wb') as file:\n",
    "    pickle.dump(svm_pipeline, file)\n",
    "print_decorative_log(\"SVM Model Exported\", Fore.GREEN)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 14:13:27.522 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\MUSA\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# Define the channel pairs and their joined names\n",
    "channel_pairs = [\n",
    "    ['EEG Fp1', 'EEG F7'], ['EEG F7', 'EEG T3'], ['EEG T3', 'EEG T5'], ['EEG T5', 'EEG O1'],\n",
    "    ['EEG Fp1', 'EEG F3'], ['EEG C3', 'EEG F3'], ['EEG F3', 'EEG O1'], ['EEG Fp2', 'EEG F4'],\n",
    "    ['EEG F4', 'EEG C4'], ['EEG C4', 'EEG P4'], ['EEG P4', 'EEG O2'], ['EEG Fp2', 'EEG F8'],\n",
    "    ['EEG F8', 'EEG T4'], ['EEG T4', 'EEG T6'], ['EEG T6', 'EEG O2']\n",
    "]\n",
    "channel_pairs_joined = ['{}-{}'.format(pair[0], pair[1]) for pair in channel_pairs]\n",
    "\n",
    "# Load the pre-trained machine learning model\n",
    "model_file = 'epilepsy_prediction_model.pkl'\n",
    "model = joblib.load(model_file)\n",
    "\n",
    "# Define the target sampling rate\n",
    "target_sampling_rate = 256\n",
    "\n",
    "# Function to compute cepstrum_mel\n",
    "def compute_cepstrum_mel(data, sfreq, n_mfcc=20):\n",
    "    mfccs = librosa.feature.mfcc(y=data, sr=sfreq, n_mfcc=n_mfcc)\n",
    "    return mfccs\n",
    "\n",
    "# Function to preprocess the raw data\n",
    "def preprocess_raw(raw):\n",
    "    # Preprocessing steps...\n",
    "\n",
    "    print_decorative_log(\"Starting Preprocessing Sequence\", Fore.GREEN)\n",
    "               \n",
    "    # Select the desired channels from channel pairs which resemble the bipolar longitudinal  channels of 10-20 system \n",
    "    selected_channels = []\n",
    "    [selected_channels.extend(pair) for pair in channel_pairs if pair not in selected_channels]\n",
    "\n",
    "    selected_channels = list(OrderedDict.fromkeys(selected_channels))\n",
    "    selected_channels.append('2')\n",
    "\n",
    "    #Drop extra channels\n",
    "    # Check the number of channels\n",
    "    #if len(raw.ch_names) > 35:\n",
    "    for i, channel_name in enumerate(raw.ch_names):\n",
    "        if 'EEG FP2' in channel_name:\n",
    "            raw.rename_channels({channel_name: 'EEG Fp2'})\n",
    "    # Drop channels not found in the desired channel list\n",
    "    channels_to_drop = [channel_name for channel_name in raw.ch_names if channel_name not in selected_channels]\n",
    "    raw.drop_channels(channels_to_drop)\n",
    "    print_decorative_log(\"Extra Channels Dropped ... \", Fore.RED)\n",
    "\n",
    "    # Reorder the channels to match the standard ordering for the dataset\n",
    "    channels_order = selected_channels\n",
    "    # Reorder channels\n",
    "    raw = raw.pick(channels_order)\n",
    "    print_decorative_log(\"Channels Reordered ... \", Fore.YELLOW)       \n",
    "    # Set the channel type for '2' to 'ecg'\n",
    "    raw.set_channel_types({'2': 'ecg'})\n",
    "    \n",
    "    print_decorative_log(\"ECG Channel Selected ... \", Fore.YELLOW)\n",
    "\n",
    "    # Filtering to remove slow drifts\n",
    "    filt_raw = raw.copy().filter(l_freq=1.0, h_freq=None)\n",
    "    print_decorative_log(\"Slow drifts removed ... \", Fore.YELLOW)\n",
    "\n",
    "    # Apply ICA to remove ECG artifacts\n",
    "\n",
    "    ica = mne.preprocessing.ICA(n_components=15, max_iter=\"auto\", random_state=97)\n",
    "    ica.fit(filt_raw)\n",
    "    ica.exclude = []\n",
    "    ecg_indices, ecg_scores = ica.find_bads_ecg(raw, method=\"correlation\", threshold=\"auto\")\n",
    "    ica.exclude = ecg_indices\n",
    "\n",
    "    reconst_raw = raw.copy()\n",
    "    ica.apply(reconst_raw)\n",
    "\n",
    "    print_decorative_log(\"ECG Artificats Removed... \", Fore.YELLOW)\n",
    "\n",
    "\n",
    "    # Perform bipolar longitudinal referencing\n",
    "    anodes = []\n",
    "    cathodes = []\n",
    "    for pair in channel_pairs:\n",
    "        anodes.append(pair[0])\n",
    "        cathodes.append(pair[1])\n",
    "\n",
    "    raw_bip_ref = mne.set_bipolar_reference(reconst_raw, anode=anodes, cathode=cathodes)\n",
    "    raw_bip_ref_ch = raw_bip_ref.copy().pick_channels(channel_pairs_joined)\n",
    "    print_decorative_log(\"Bipolar Referencing Done ... \", Fore.YELLOW)\n",
    "    raw_clean = mne.preprocessing.oversampled_temporal_projection(raw_bip_ref_ch)\n",
    "    raw_clean.filter(0.0, 40.0)\n",
    "    print_decorative_log(\"Smoothing & Filtering Done ... \", Fore.YELLOW)\n",
    "\n",
    "    return raw_clean\n",
    "\n",
    "\n",
    "# Function to simulate streaming data and make predictions\n",
    "def simulate_streaming_data(raw, start_time, end_time):\n",
    "    st.write(\"Starting Simulation\")\n",
    "\n",
    "    # Crop the raw data to the specified start and end time\n",
    "    raw.crop(tmin=start_time, tmax=end_time)\n",
    "\n",
    "    # Preprocess the raw data\n",
    "    preprocessed_raw = preprocess_raw(raw)\n",
    "\n",
    "    # Get the data and the corresponding time vector\n",
    "    data = preprocessed_raw.get_data()\n",
    "    time = preprocessed_raw.times\n",
    "\n",
    "    # Define the window size for frame sampling\n",
    "    window_size = 10  # Window size in seconds\n",
    "\n",
    "    # Calculate the number of samples in the window\n",
    "    window_samples = int(window_size * target_sampling_rate)\n",
    "\n",
    "    # Calculate the number of frames\n",
    "    num_frames = int(len(data[0]) / window_samples)\n",
    "\n",
    "    # Iterate over the frames\n",
    "    for frame_idx in range(num_frames):\n",
    "        # Calculate the start and end sample indices for the current frame\n",
    "        start_idx = frame_idx * window_samples\n",
    "        end_idx = start_idx + window_samples\n",
    "\n",
    "        # Extract the frame data for all channels\n",
    "        frame_data = data[:, start_idx:end_idx]\n",
    "\n",
    "        # Compute mfccs\n",
    "        n_mfcc = 20  # Number of MFCC coefficients\n",
    "        cepstrum_mel_features = []\n",
    "        for channel_data in frame_data:\n",
    "            cepstrum_mel = compute_cepstrum_mel(channel_data, target_sampling_rate, n_mfcc)\n",
    "            cepstrum_mel_features.append(cepstrum_mel)\n",
    "        cepstral_features = np.concatenate(cepstrum_mel_features, axis=0)\n",
    "\n",
    "        # Convert features to DataFrame\n",
    "        frame_df = pd.DataFrame(cepstral_features.T, columns=[f'mfcc_{i}' for i in range(1, n_mfcc + 1)])\n",
    "\n",
    "        # Apply feature scaling to the latest frame data\n",
    "        scaler = StandardScaler()\n",
    "        frame_scaled = scaler.fit_transform(frame_df)\n",
    "\n",
    "        # Make prediction using the pre-trained model\n",
    "        prediction = model.predict(frame_scaled)[0]\n",
    "\n",
    "        # Map the predicted label to the corresponding class\n",
    "        class_mapping = {0: 'pre-ictal', 1: 'ictal', 2: 'post-ictal', 3: 'normal'}\n",
    "        predicted_class = class_mapping[prediction]\n",
    "\n",
    "        # Display the streaming data and classification result\n",
    "        st.subheader(\"Streaming 10 secs\")\n",
    "        st.info(f\"Classification Result: {predicted_class}\")\n",
    "        st.write(\"--------------------------------\")\n",
    "\n",
    "# Streamlit app\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def load_data(file_path):\n",
    "    raw = mne.io.read_raw_edf(file_path)\n",
    "    return raw\n",
    "\n",
    "def main():\n",
    "    st.title(\"EDF Streaming Data Classification\")\n",
    "\n",
    "    # File upload and user input\n",
    "    uploaded_file = st.file_uploader(\"Upload EDF file\", type=[\"edf\"])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        raw = load_data(uploaded_file)\n",
    "\n",
    "        start_time = st.number_input(\"Start Time (in seconds)\", min_value=0.0, max_value=raw.times[-1], value=0.0)\n",
    "        end_time = st.number_input(\"End Time (in seconds)\", min_value=start_time, max_value=raw.times[-1], value=raw.times[-1])\n",
    "\n",
    "        if st.button(\"Start Classification\"):\n",
    "            simulate_streaming_data(raw, start_time, end_time)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroGuard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
