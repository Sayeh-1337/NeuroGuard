{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        mfcc_1      mfcc_2      mfcc_3     mfcc_4     mfcc_5     mfcc_6  \\\n",
      "0 -1056.156930  100.706891   84.862294  61.978023  36.441811  12.850290   \n",
      "1 -1053.566761  104.572517   89.251661  66.992090  41.908143  18.372516   \n",
      "2 -1054.493806  103.244414   87.882304  65.582452  40.487623  16.988013   \n",
      "3 -1044.344650  116.555050   98.293259  71.862921  42.277698  14.831793   \n",
      "4 -1037.935844  124.927492  104.753948  75.632052  43.180481  13.306042   \n",
      "\n",
      "     mfcc_7     mfcc_8     mfcc_9    mfcc_10  ...  mfcc_292   mfcc_293  \\\n",
      "0 -5.030501 -15.043214 -16.971785 -12.373160  ...  1.097410   8.823732   \n",
      "1  0.060017 -10.797882 -13.775119 -10.161991  ...  1.919385   9.382641   \n",
      "2 -1.245174 -12.006527 -14.909664 -11.281917  ...  3.386919  10.842312   \n",
      "3 -6.079423 -17.866068 -20.164598 -14.694518  ...  3.274818  10.696498   \n",
      "4 -9.129471 -21.325330 -22.999589 -16.203308  ...  3.561111  10.834448   \n",
      "\n",
      "    mfcc_294   mfcc_295   mfcc_296  mfcc_297  mfcc_298  mfcc_299  mfcc_300  \\\n",
      "0  12.765254  12.431488   8.617166  3.001667 -2.442837 -6.052161 -6.912017   \n",
      "1  13.133085  12.623349   8.589544  2.681310 -3.111127 -7.058276 -8.153845   \n",
      "2  14.541879  13.964836   9.865696  3.895486 -1.973261 -6.041656 -7.334056   \n",
      "3  14.470068  14.095678  10.327999  4.790366 -0.600028 -4.218610 -5.167414   \n",
      "4  14.646522  14.443124  10.890768  5.521836  0.187959 -3.509877 -4.654400   \n",
      "\n",
      "   label  \n",
      "0    0.0  \n",
      "1    0.0  \n",
      "2    0.0  \n",
      "3    0.0  \n",
      "4    0.0  \n",
      "\n",
      "[5 rows x 301 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('merged_dataset.csv')\n",
    "# Define the column names\n",
    "columns = ['mfcc_' + str(i) for i in range(1, 301)] + ['label']\n",
    "\n",
    "# Assign the column names to the DataFrame\n",
    "df.columns = columns\n",
    "\n",
    "# Print the DataFrame with headers\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424782 entries, 0 to 424781\n",
      "Columns: 301 entries, mfcc_1 to label\n",
      "dtypes: float64(301)\n",
      "memory usage: 975.5 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>mfcc_7</th>\n",
       "      <th>mfcc_8</th>\n",
       "      <th>mfcc_9</th>\n",
       "      <th>mfcc_10</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_292</th>\n",
       "      <th>mfcc_293</th>\n",
       "      <th>mfcc_294</th>\n",
       "      <th>mfcc_295</th>\n",
       "      <th>mfcc_296</th>\n",
       "      <th>mfcc_297</th>\n",
       "      <th>mfcc_298</th>\n",
       "      <th>mfcc_299</th>\n",
       "      <th>mfcc_300</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "      <td>424782.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-957.251790</td>\n",
       "      <td>73.429367</td>\n",
       "      <td>63.587957</td>\n",
       "      <td>49.229162</td>\n",
       "      <td>32.865257</td>\n",
       "      <td>17.208224</td>\n",
       "      <td>4.588648</td>\n",
       "      <td>-3.502608</td>\n",
       "      <td>-6.652877</td>\n",
       "      <td>-5.502739</td>\n",
       "      <td>...</td>\n",
       "      <td>2.052021</td>\n",
       "      <td>5.984551</td>\n",
       "      <td>8.344310</td>\n",
       "      <td>8.635752</td>\n",
       "      <td>7.017027</td>\n",
       "      <td>4.160127</td>\n",
       "      <td>0.995210</td>\n",
       "      <td>-1.579928</td>\n",
       "      <td>-2.944798</td>\n",
       "      <td>2.964808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>93.061677</td>\n",
       "      <td>28.003700</td>\n",
       "      <td>23.140265</td>\n",
       "      <td>16.322959</td>\n",
       "      <td>9.424632</td>\n",
       "      <td>5.998543</td>\n",
       "      <td>7.846342</td>\n",
       "      <td>9.642355</td>\n",
       "      <td>9.411135</td>\n",
       "      <td>7.427699</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479999</td>\n",
       "      <td>3.319391</td>\n",
       "      <td>4.151845</td>\n",
       "      <td>3.867859</td>\n",
       "      <td>2.713942</td>\n",
       "      <td>1.767335</td>\n",
       "      <td>2.360405</td>\n",
       "      <td>3.208475</td>\n",
       "      <td>3.376831</td>\n",
       "      <td>0.284380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1131.370850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.052197</td>\n",
       "      <td>-10.106800</td>\n",
       "      <td>-31.239292</td>\n",
       "      <td>-47.488093</td>\n",
       "      <td>-46.642887</td>\n",
       "      <td>-34.282181</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.140624</td>\n",
       "      <td>-3.907965</td>\n",
       "      <td>-3.207446</td>\n",
       "      <td>-3.123274</td>\n",
       "      <td>-3.264276</td>\n",
       "      <td>-8.396230</td>\n",
       "      <td>-15.581895</td>\n",
       "      <td>-18.701742</td>\n",
       "      <td>-18.761260</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1047.499379</td>\n",
       "      <td>52.860956</td>\n",
       "      <td>46.600853</td>\n",
       "      <td>37.090646</td>\n",
       "      <td>25.264859</td>\n",
       "      <td>12.809885</td>\n",
       "      <td>-0.119585</td>\n",
       "      <td>-9.119909</td>\n",
       "      <td>-12.133904</td>\n",
       "      <td>-10.098260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420948</td>\n",
       "      <td>3.484893</td>\n",
       "      <td>4.846934</td>\n",
       "      <td>5.357046</td>\n",
       "      <td>4.922851</td>\n",
       "      <td>3.010286</td>\n",
       "      <td>-0.536171</td>\n",
       "      <td>-3.863631</td>\n",
       "      <td>-5.477259</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-978.547489</td>\n",
       "      <td>76.241309</td>\n",
       "      <td>66.240453</td>\n",
       "      <td>51.352780</td>\n",
       "      <td>34.086228</td>\n",
       "      <td>17.131187</td>\n",
       "      <td>5.559055</td>\n",
       "      <td>-2.831372</td>\n",
       "      <td>-6.571349</td>\n",
       "      <td>-5.756559</td>\n",
       "      <td>...</td>\n",
       "      <td>1.957027</td>\n",
       "      <td>5.740892</td>\n",
       "      <td>7.847505</td>\n",
       "      <td>8.383230</td>\n",
       "      <td>6.992698</td>\n",
       "      <td>4.082627</td>\n",
       "      <td>1.124616</td>\n",
       "      <td>-1.468043</td>\n",
       "      <td>-3.064989</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-885.080499</td>\n",
       "      <td>91.779168</td>\n",
       "      <td>79.169843</td>\n",
       "      <td>60.863535</td>\n",
       "      <td>40.169603</td>\n",
       "      <td>21.809376</td>\n",
       "      <td>10.334772</td>\n",
       "      <td>3.077508</td>\n",
       "      <td>-0.563506</td>\n",
       "      <td>-0.857388</td>\n",
       "      <td>...</td>\n",
       "      <td>3.498399</td>\n",
       "      <td>8.364641</td>\n",
       "      <td>11.669916</td>\n",
       "      <td>11.833003</td>\n",
       "      <td>9.114500</td>\n",
       "      <td>5.216890</td>\n",
       "      <td>2.633316</td>\n",
       "      <td>0.889726</td>\n",
       "      <td>-0.359829</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-405.694693</td>\n",
       "      <td>216.038255</td>\n",
       "      <td>173.814956</td>\n",
       "      <td>116.897620</td>\n",
       "      <td>67.363842</td>\n",
       "      <td>41.974007</td>\n",
       "      <td>27.413303</td>\n",
       "      <td>21.702301</td>\n",
       "      <td>18.714559</td>\n",
       "      <td>17.117090</td>\n",
       "      <td>...</td>\n",
       "      <td>22.024933</td>\n",
       "      <td>26.631968</td>\n",
       "      <td>27.885411</td>\n",
       "      <td>26.992258</td>\n",
       "      <td>21.979536</td>\n",
       "      <td>15.633528</td>\n",
       "      <td>13.494096</td>\n",
       "      <td>11.531253</td>\n",
       "      <td>9.691933</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              mfcc_1         mfcc_2         mfcc_3         mfcc_4  \\\n",
       "count  424782.000000  424782.000000  424782.000000  424782.000000   \n",
       "mean     -957.251790      73.429367      63.587957      49.229162   \n",
       "std        93.061677      28.003700      23.140265      16.322959   \n",
       "min     -1131.370850       0.000000       0.000000       0.000000   \n",
       "25%     -1047.499379      52.860956      46.600853      37.090646   \n",
       "50%      -978.547489      76.241309      66.240453      51.352780   \n",
       "75%      -885.080499      91.779168      79.169843      60.863535   \n",
       "max      -405.694693     216.038255     173.814956     116.897620   \n",
       "\n",
       "              mfcc_5         mfcc_6         mfcc_7         mfcc_8  \\\n",
       "count  424782.000000  424782.000000  424782.000000  424782.000000   \n",
       "mean       32.865257      17.208224       4.588648      -3.502608   \n",
       "std         9.424632       5.998543       7.846342       9.642355   \n",
       "min        -8.052197     -10.106800     -31.239292     -47.488093   \n",
       "25%        25.264859      12.809885      -0.119585      -9.119909   \n",
       "50%        34.086228      17.131187       5.559055      -2.831372   \n",
       "75%        40.169603      21.809376      10.334772       3.077508   \n",
       "max        67.363842      41.974007      27.413303      21.702301   \n",
       "\n",
       "              mfcc_9        mfcc_10  ...       mfcc_292       mfcc_293  \\\n",
       "count  424782.000000  424782.000000  ...  424782.000000  424782.000000   \n",
       "mean       -6.652877      -5.502739  ...       2.052021       5.984551   \n",
       "std         9.411135       7.427699  ...       2.479999       3.319391   \n",
       "min       -46.642887     -34.282181  ...      -7.140624      -3.907965   \n",
       "25%       -12.133904     -10.098260  ...       0.420948       3.484893   \n",
       "50%        -6.571349      -5.756559  ...       1.957027       5.740892   \n",
       "75%        -0.563506      -0.857388  ...       3.498399       8.364641   \n",
       "max        18.714559      17.117090  ...      22.024933      26.631968   \n",
       "\n",
       "            mfcc_294       mfcc_295       mfcc_296       mfcc_297  \\\n",
       "count  424782.000000  424782.000000  424782.000000  424782.000000   \n",
       "mean        8.344310       8.635752       7.017027       4.160127   \n",
       "std         4.151845       3.867859       2.713942       1.767335   \n",
       "min        -3.207446      -3.123274      -3.264276      -8.396230   \n",
       "25%         4.846934       5.357046       4.922851       3.010286   \n",
       "50%         7.847505       8.383230       6.992698       4.082627   \n",
       "75%        11.669916      11.833003       9.114500       5.216890   \n",
       "max        27.885411      26.992258      21.979536      15.633528   \n",
       "\n",
       "            mfcc_298       mfcc_299       mfcc_300          label  \n",
       "count  424782.000000  424782.000000  424782.000000  424782.000000  \n",
       "mean        0.995210      -1.579928      -2.944798       2.964808  \n",
       "std         2.360405       3.208475       3.376831       0.284380  \n",
       "min       -15.581895     -18.701742     -18.761260       0.000000  \n",
       "25%        -0.536171      -3.863631      -5.477259       3.000000  \n",
       "50%         1.124616      -1.468043      -3.064989       3.000000  \n",
       "75%         2.633316       0.889726      -0.359829       3.000000  \n",
       "max        13.494096      11.531253       9.691933       3.000000  \n",
       "\n",
       "[8 rows x 301 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MUSA\\AppData\\Local\\Temp\\ipykernel_24236\\2193183711.py:5: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "Num GPUs Available:  1\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "<tensorflow.python.client.session.Session object at 0x000001E8157910A0>\n"
     ]
    }
   ],
   "source": [
    "# Configure TensorFlow to use GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# Print GPU devices\n",
    "tf.test.gpu_device_name()\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "for gpu in gpus:\n",
    "    print(gpu)\n",
    "\n",
    "# Print session data\n",
    "print(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MUSA\\anaconda3\\envs\\NeuroGuard-ML\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:576: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\MUSA\\anaconda3\\envs\\NeuroGuard-ML\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:588: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\MUSA\\anaconda3\\envs\\NeuroGuard-ML\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:600: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Random Forest Accuracy: 0.9236201843285426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.11      0.90      0.20       512\n",
      "         1.0       0.37      0.89      0.53       540\n",
      "         2.0       0.18      0.90      0.30       506\n",
      "         3.0       1.00      0.92      0.96     83399\n",
      "\n",
      "    accuracy                           0.92     84957\n",
      "   macro avg       0.42      0.90      0.50     84957\n",
      "weighted avg       0.99      0.92      0.95     84957\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('label', axis=1).values # Features\n",
    "#y = df['label'].values  # Labels\n",
    "y = df['label'].values # Labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a standard scaler\n",
    "scaler = StandardScaler()\n",
    "# Normalize the feature values using StandardScaler\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of Balanced Random Forest classifier\n",
    "brf = BalancedRandomForestClassifier(n_estimators=100, random_state=42)  # Adjust the number of estimators as needed\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "brf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = brf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = brf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Balanced Random Forest Accuracy: {accuracy}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Export the trained SVM model to a pickle file\n",
    "with open('epilepsy_prediction_model.pkl', 'wb') as file:\n",
    "    pickle.dump(brf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m####################\n",
      "# Dataset Splitted #\n",
      "####################\u001b[0m\n",
      "\u001b[33m###################################\n",
      "# Normalization & Standardization #\n",
      "###################################\u001b[0m\n",
      "\u001b[33m######################\n",
      "# SMOTE Oversampling #\n",
      "######################\u001b[0m\n",
      "Model: SVM\n",
      "Parameter Grid: {'svm__C': [0.1, 1, 10], 'svm__kernel': ['linear', 'rbf']}\n",
      "\n",
      "\u001b[33m###############\n",
      "# Grid Search #\n",
      "###############\u001b[0m\n",
      "\u001b[33m############\n",
      "# Training #\n",
      "############\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from colorama import Fore, Style\n",
    "\n",
    "def print_decorative_log(message, color=Fore.BLUE, style=Style.RESET_ALL):\n",
    "    line_length = len(message) + 4  # Length of the message plus padding on both sides\n",
    "    decorative_line = \"#\" * line_length\n",
    "    print(color + decorative_line)\n",
    "    print(f\"# {message} #\")\n",
    "    print(decorative_line + style)\n",
    "\n",
    "# Load the dataset\n",
    "X = df.drop('label', axis=1).values # Features\n",
    "#y = df['label'].values  # Labels\n",
    "y = df['label'].values # Labels\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print_decorative_log(\"Dataset Splitted\", Fore.GREEN)\n",
    "\n",
    "# Define a standard scaler\n",
    "print_decorative_log(\"Normalization & Standardization\", Fore.YELLOW)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define SMOTE oversampling\n",
    "print_decorative_log(\"SMOTE Oversampling\", Fore.YELLOW)\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Define the models and their respective parameter grids\n",
    "models = [\n",
    "    {\n",
    "        'name': 'SVM',\n",
    "        'pipeline': ImbPipeline([\n",
    "            ('scaler', scaler),\n",
    "            ('smote', smote),\n",
    "            ('svm', SVC(random_state=42))\n",
    "        ]),\n",
    "        'param_grid': {'svm__C': [0.1, 1, 10], 'svm__kernel': ['linear', 'rbf']}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'pipeline': ImbPipeline([\n",
    "            ('scaler', scaler),\n",
    "            ('smote', smote),\n",
    "            ('rf', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        'param_grid': {'rf__n_estimators': [100, 200, 500], 'rf__max_depth': [None, 5, 10]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'AdaBoost',\n",
    "        'pipeline': ImbPipeline([\n",
    "            ('scaler', scaler),\n",
    "            ('smote', smote),\n",
    "            ('adaboost', AdaBoostClassifier(random_state=42))\n",
    "        ]),\n",
    "        'param_grid': {'adaboost__n_estimators': [50, 100, 200], 'adaboost__learning_rate': [0.1, 0.5, 1]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGBoost',\n",
    "        'pipeline': ImbPipeline([\n",
    "            ('scaler', scaler),\n",
    "            ('smote', smote),\n",
    "            ('xgboost', XGBClassifier(random_state=42))\n",
    "        ]),\n",
    "        'param_grid': {'xgboost__n_estimators': [100, 200, 500], 'xgboost__learning_rate': [0.1, 0.5, 1]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Balanced Random Forest',\n",
    "        'pipeline': ImbPipeline([\n",
    "            ('scaler', scaler),\n",
    "            ('smote', smote),\n",
    "            ('brf', BalancedRandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        'param_grid': {'brf__n_estimators': [100, 200, 500], 'brf__max_depth': [None, 5, 10]}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Variables to store the best model and its performance\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate through each model\n",
    "for model in models:\n",
    "    print(f\"Model: {model['name']}\")\n",
    "    print(\"Parameter Grid:\", model['param_grid'])\n",
    "    print()\n",
    "\n",
    "    print_decorative_log(\"Grid Search\", Fore.YELLOW)\n",
    "    # Perform Grid Search for the current model\n",
    "    grid_search = GridSearchCV(model['pipeline'], model['param_grid'], scoring='accuracy', cv=5)\n",
    "    print_decorative_log(\"Training\", Fore.YELLOW)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    print_decorative_log(\"Predicting\", Fore.YELLOW)\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "    \n",
    "    print_decorative_log(\"Evaluation\", Fore.YELLOW)\n",
    "    # Print the classification report\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Accuracy:\", grid_search.best_score_)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print()\n",
    "\n",
    "    # Check if this model has the best accuracy so far\n",
    "    if grid_search.best_score_ > best_accuracy:\n",
    "        best_accuracy = grid_search.best_score_\n",
    "        best_model = model['name']\n",
    "\n",
    "# Print the best model\n",
    "print(f\"The best model is: {best_model} with accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop('label', axis=1).values.astype(np.float32)  # Features\n",
    "#y = df['label'].values  # Labels\n",
    "y = df['label'].values.astype(np.float32)  # Labels\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# SVM Model\n",
    "svm_model = SVC()\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', svm_model)\n",
    "])\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', rf_model)\n",
    "])\n",
    "\n",
    "# RNN Model\n",
    "def create_rnn_model():\n",
    "    strategy = tf.distribute.OneDeviceStrategy('GPU:0')  # Use the first GPU\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "        model.add(LSTM(32, return_sequences=False))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN Model\n",
    "def create_cnn_model():\n",
    "    strategy = tf.distribute.OneDeviceStrategy('GPU:0')  # Use the first GPU\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap Keras models for use in scikit-learn\n",
    "rnn_model = KerasClassifier(build_fn=create_rnn_model, epochs=10, batch_size=10, verbose=1)\n",
    "cnn_model = KerasClassifier(build_fn=create_cnn_model, epochs=10, batch_size=10, verbose=1)\n",
    "\n",
    "# Fit and evaluate models\n",
    "models = {#'SVM': svm_pipeline,\n",
    "          #'Random Forest': rf_pipeline,\n",
    "          'RNN': rnn_model,\n",
    "          'CNN': cnn_model}\n",
    "\n",
    "accuracy_values = []\n",
    "reports = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name in ['RNN', 'CNN']:\n",
    "        \n",
    "        model.fit(X_train, to_categorical(y_train))\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    accuracy_values.append(accuracy)\n",
    "\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    reports.append(report)\n",
    "\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    print(report)\n",
    "    print()\n",
    "\n",
    "model_names = list(models.keys())\n",
    "plt.bar(model_names, accuracy_values)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.show()\n",
    "\n",
    "for model_name, report in zip(model_names, reports):\n",
    "    print(f\"Classification Report for {model_name}:\")\n",
    "    print(report)\n",
    "    print()\n",
    "\n",
    "best_model_name = max(models, key=lambda name: models[name].score(X_test, y_test))\n",
    "best_model = models[best_model_name]\n",
    "print(f\"Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configure TensorFlow to use GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/path/to/dataset.csv')\n",
    "\n",
    "# Define the column names\n",
    "columns = ['mfcc_' + str(i) for i in range(1, 301)] + ['label']\n",
    "\n",
    "# Assign the column names to the DataFrame\n",
    "df.columns = columns\n",
    "\n",
    "# Print the DataFrame with headers\n",
    "print(df.head())\n",
    "\n",
    "X = df.drop('label', axis=1).values  # Features\n",
    "y = df['label'].values  # Labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# SVM Pipeline\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Random Forest Pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# RNN Model\n",
    "def create_rnn_model():\n",
    "    strategy = tf.distribute.OneDeviceStrategy('GPU:0')  # Use the first GPU\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "        model.add(LSTM(32, return_sequences=False))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN Model\n",
    "def create_cnn_model():\n",
    "    strategy = tf.distribute.OneDeviceStrategy('GPU:0')  # Use the first GPU\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap Keras models for use in scikit-learn\n",
    "rnn_model = KerasClassifier(build_fn=create_rnn_model, epochs=10, batch_size=10, verbose=1)\n",
    "cnn_model = KerasClassifier(build_fn=create_cnn_model, epochs=10, batch_size=10, verbose=1)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "svm_param_grid = {'svm__C': [1, 10], 'svm__gamma': [0.001, 0.01]}\n",
    "rf_param_grid = {'rf__n_estimators': [50, 100], 'rf__max_depth': [10, 20]}\n",
    "rnn_param_grid = {'batch_size': [10, 20], 'epochs': [10, 20]}\n",
    "cnn_param_grid = {'batch_size': [10, 20], 'epochs': [10, 20]}\n",
    "\n",
    "# Define cross-validation folds\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grids = {}\n",
    "for model_name, pipeline, param_grid in [('SVM', svm_pipeline, svm_param_grid),\n",
    "                                         ('RandomForest', rf_pipeline, rf_param_grid),\n",
    "                                         ('RNN', rnn_model, rnn_param_grid),\n",
    "                                         ('CNN', cnn_model, cnn_param_grid)]:\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, n_jobs=-1, cv=cv, verbose=2)\n",
    "    grid_search.fit(X_train, to_categorical(y_train) if model_name in ['RNN', 'CNN'] else y_train)\n",
    "    grids[model_name] = grid_search\n",
    "\n",
    "# Evaluate models\n",
    "for model_name, grid in grids.items():\n",
    "    if model_name in ['RNN', 'CNN']:\n",
    "        y_pred = grid.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    else:\n",
    "        y_pred = grid.predict(X_test)\n",
    "    print(f\"{model_name} Best Params: {grid.best_params_}\")\n",
    "    print(f\"{model_name} Accuracy: {grid.best_score_}\")\n",
    "    print(f\"{model_name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = max(grids, key=lambda name: grids[name].best_score_)\n",
    "best_model = grids[best_model_name].best_estimator_\n",
    "print(f\"Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop('label', axis=1).values  # Features\n",
    "# y = df['label'].values  # Labels\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a standard scaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # SVM Pipeline\n",
    "# svm_pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('svm', SVC())\n",
    "# ])\n",
    "\n",
    "# # Random Forest Pipeline\n",
    "# rf_pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('rf', RandomForestClassifier())\n",
    "# ])\n",
    "\n",
    "# # RNN Model\n",
    "# def create_rnn_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "#     model.add(LSTM(32, return_sequences=False))\n",
    "#     model.add(Dense(4, activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # CNN Model\n",
    "# def create_cnn_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     model.add(Dense(4, activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Wrap Keras models for use in scikit-learn\n",
    "# rnn_model = KerasClassifier(build_fn=create_rnn_model, epochs=10, batch_size=10, verbose=0)\n",
    "# cnn_model = KerasClassifier(build_fn=create_cnn_model, epochs=10, batch_size=10, verbose=0)\n",
    "\n",
    "# # Define parameter grid for GridSearchCV\n",
    "# svm_param_grid = {'svm__C': [1, 10], 'svm__gamma': [0.001, 0.01]}\n",
    "# rf_param_grid = {'rf__n_estimators': [50, 100], 'rf__max_depth': [10, 20]}\n",
    "# rnn_param_grid = {'batch_size': [10, 20], 'epochs': [10, 20]}\n",
    "# cnn_param_grid = {'batch_size': [10, 20], 'epochs': [10, 20]}\n",
    "\n",
    "# # Define cross-validation folds\n",
    "# cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# # Perform GridSearchCV\n",
    "# grids = {}\n",
    "# for model_name, pipeline, param_grid in [('SVM', svm_pipeline, svm_param_grid),\n",
    "#                                          ('RandomForest', rf_pipeline, rf_param_grid),\n",
    "#                                          ('RNN', rnn_model, rnn_param_grid),\n",
    "#                                          ('CNN', cnn_model, cnn_param_grid)]:\n",
    "#     grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, n_jobs=-1, cv=cv)\n",
    "#     grid_search.fit(X_train, to_categorical(y_train) if model_name in ['RNN', 'CNN'] else y_train)\n",
    "#     grids[model_name] = grid_search\n",
    "\n",
    "# # Evaluate models\n",
    "# for model_name, grid in grids.items():\n",
    "#     if model_name in ['RNN', 'CNN']:\n",
    "#         y_pred = grid.predict(X_test)\n",
    "#         y_pred = np.argmax(y_pred, axis=1)\n",
    "#     else:\n",
    "#         y_pred = grid.predict(X_test)\n",
    "#     print(f\"{model_name} Best Params: {grid.best_params_}\")\n",
    "#     print(f\"{model_name} Accuracy: {grid.best_score_}\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Select the best model\n",
    "# best_model_name = max(grids, key=lambda name: grids[name].best_score_)\n",
    "# best_model = grids[best_model_name].best_estimator_\n",
    "# print(f\"Best Model: {best_model_name}\")\n",
    "\n",
    "# # Save the model if needed\n",
    "# # best_model.model.save('best_model.h5')  # Uncomment this line to save the Keras model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroGuard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
